from kafka import KafkaConsumer, TopicPartition
from datetime import datetime
import os
import time
import config  # assuming config module is defined

class CampaignConsumer:
    def __init__(self, topic):
        self.topic = topic
        self.consumer = KafkaConsumer(self.topic, group_id='campaign_group', bootstrap_servers=config.KAFKA_SERVER, auto_offset_reset='earliest', enable_auto_commit=True)
        # Other initialization for the campaign consumer

    def send_campaign(self, data):
        # Implement your logic to send campaign
        print("Sending campaign:", data)

    def process_messages(self):
        for kafka_message in self.consumer:
            data = kafka_message.value.decode()
            self.send_campaign(data)

class DataWriterConsumer:
    def __init__(self, topic, max_record, offset_last_path):
        self.topic = topic
        self.max_record = max_record
        self.offset_last_path = offset_last_path
        self.consumer = KafkaConsumer(self.topic, group_id='data_writer_group', bootstrap_servers=config.KAFKA_SERVER, auto_offset_reset='earliest', enable_auto_commit=True)
        self.log_folder = None
        self.event = None  # assuming the event variable is defined

    def read_offsetlast(self, path):
        # Implement your logic to read the last offset from the file
        pass

    def write_data(self, data):
        # Implement your logic to write data to the file
        print("Writing data:", data)

    def get_create_log_folder(self):
        # Implement your logic to get or create the log folder
        pass

    def upload_minio(self, folder_path):
        # Implement your logic to upload folder to Minio
        print("Uploading to Minio:", folder_path)

    def process_messages(self):
        print("===Upload Topic: ", datetime.now(), self.topic)

        offset_last_get = self.read_offsetlast(self.offset_last_path)
        if offset_last_get is None:
            offset_last_get = 0

        self.log_folder = self.get_create_log_folder()
        count = 0

        print("== after process kafka message in consumer==", datetime.now(), self.topic)

        for kafka_message in self.consumer:
            t1 = time.time()
            print("Consumer process message: ", datetime.now(), self.topic, kafka_message.value.decode())

            if self.event.is_set():
                break

            if not os.path.isdir(self.log_folder):
                self.log_folder = self.get_create_log_folder()

            data = kafka_message.value.decode()
            self.write_data(data)
            t2 = time.time()
            print("Write_data: ", datetime.now(), self.topic, t2 - t1)
            count += 1

            if count == self.max_record or kafka_message.offset == self.consumer.position(kafka_message.partition).offset - 1:
                self.upload_minio(self.log_folder)
                count = 0
                self.log_folder = self.get_create_log_folder()
                self.write_data(self.consumer.position(kafka_message.partition).offset)
                t3 = time.time()
                print("Write_data offset: ", datetime.now(), self.topic, t3 - t2)

            if self.consumer.position(kafka_message.partition).offset > self.consumer.end_offsets([kafka_message.partition])[kafka_message.partition] - 1:
                self.send_campaigmgt(data)
            
            time.sleep(0.01)

# Usage
campaign_consumer = CampaignConsumer("your_topic")
data_writer_consumer = DataWriterConsumer("your_topic", 100, "your_offset_last_path")

# Start the consumers in separate threads or processes
campaign_consumer_thread = Thread(target=campaign_consumer.process_messages)
data_writer_consumer_thread = Thread(target=data_writer_consumer.process_messages)

campaign_consumer_thread.start()
data_writer_consumer_thread.start()

campaign_consumer_thread.join()
data_writer_consumer_thread.join()


# Usage
data_writer_consumer = DataWriterConsumer("your_topic", 100, "your_offset_last_path")

# Start the consumer in a separate thread or process
data_writer_consumer_thread = Thread(target=data_writer_consumer.process_messages)
data_writer_consumer_thread.start()

data_writer_consumer_thread.join()



Exception in thread mm_click:
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/threading.py", line 926, in _bootstrap_inner
    self.run()
  File "/app/process_upload.py", line 12, in run
    upload_event.upload_topic_test()
  File "/app/upload_event_kafka_minio.py", line 114, in upload_topic_test
    consumer_write_data= KafkaConsumer(bootstrap_servers= config.KAFKA_SERVER, group_id='write_data_group_'+ self.topic, auto_offset_reset='earliest', enable_auto_commit= True)
  File "/usr/local/lib/python3.7/site-packages/kafka/consumer/group.py", line 356, in __init__
    self._client = KafkaClient(metrics=self._metrics, **self.config)
  File "/usr/local/lib/python3.7/site-packages/kafka/client_async.py", line 244, in __init__
    self.config['api_version'] = self.check_version(timeout=check_timeout)
  File "/usr/local/lib/python3.7/site-packages/kafka/client_async.py", line 927, in check_version
    raise Errors.NoBrokersAvailable()
kafka.errors.NoBrokersAvailable: NoBrokersAvailable




