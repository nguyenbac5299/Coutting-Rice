from kafka import KafkaConsumer, TopicPartition
from datetime import datetime
import os
import time
import config  # assuming config module is defined

class CampaignConsumer:
    def __init__(self, topic):
        self.topic = topic
        self.consumer = KafkaConsumer(self.topic, group_id='campaign_group', bootstrap_servers=config.KAFKA_SERVER, auto_offset_reset='earliest', enable_auto_commit=True)
        # Other initialization for the campaign consumer

    def send_campaign(self, data):
        # Implement your logic to send campaign
        print("Sending campaign:", data)

    def process_messages(self):
        for kafka_message in self.consumer:
            data = kafka_message.value.decode()
            self.send_campaign(data)

class DataWriterConsumer:
    def __init__(self, topic, max_record, offset_last_path):
        self.topic = topic
        self.max_record = max_record
        self.offset_last_path = offset_last_path
        self.consumer = KafkaConsumer(self.topic, group_id='data_writer_group', bootstrap_servers=config.KAFKA_SERVER, auto_offset_reset='earliest', enable_auto_commit=True)
        self.log_folder = None
        self.event = None  # assuming the event variable is defined

    def read_offsetlast(self, path):
        # Implement your logic to read the last offset from the file
        pass

    def write_data(self, data):
        # Implement your logic to write data to the file
        print("Writing data:", data)

    def get_create_log_folder(self):
        # Implement your logic to get or create the log folder
        pass

    def upload_minio(self, folder_path):
        # Implement your logic to upload folder to Minio
        print("Uploading to Minio:", folder_path)

    def process_messages(self):
        print("===Upload Topic: ", datetime.now(), self.topic)

        offset_last_get = self.read_offsetlast(self.offset_last_path)
        if offset_last_get is None:
            offset_last_get = 0

        self.log_folder = self.get_create_log_folder()
        count = 0

        print("== after process kafka message in consumer==", datetime.now(), self.topic)

        for kafka_message in self.consumer:
            t1 = time.time()
            print("Consumer process message: ", datetime.now(), self.topic, kafka_message.value.decode())

            if self.event.is_set():
                break

            if not os.path.isdir(self.log_folder):
                self.log_folder = self.get_create_log_folder()

            data = kafka_message.value.decode()
            self.write_data(data)
            t2 = time.time()
            print("Write_data: ", datetime.now(), self.topic, t2 - t1)
            count += 1

            if count == self.max_record or kafka_message.offset == self.consumer.position(kafka_message.partition).offset - 1:
                self.upload_minio(self.log_folder)
                count = 0
                self.log_folder = self.get_create_log_folder()
                self.write_data(self.consumer.position(kafka_message.partition).offset)
                t3 = time.time()
                print("Write_data offset: ", datetime.now(), self.topic, t3 - t2)

            if self.consumer.position(kafka_message.partition).offset > self.consumer.end_offsets([kafka_message.partition])[kafka_message.partition] - 1:
                self.send_campaigmgt(data)
            
            time.sleep(0.01)

# Usage
campaign_consumer = CampaignConsumer("your_topic")
data_writer_consumer = DataWriterConsumer("your_topic", 100, "your_offset_last_path")

# Start the consumers in separate threads or processes
campaign_consumer_thread = Thread(target=campaign_consumer.process_messages)
data_writer_consumer_thread = Thread(target=data_writer_consumer.process_messages)

campaign_consumer_thread.start()
data_writer_consumer_thread.start()

campaign_consumer_thread.join()
data_writer_consumer_thread.join()


# Usage
data_writer_consumer = DataWriterConsumer("your_topic", 100, "your_offset_last_path")

# Start the consumer in a separate thread or process
data_writer_consumer_thread = Thread(target=data_writer_consumer.process_messages)
data_writer_consumer_thread.start()

data_writer_consumer_thread.join()



from kafka import KafkaConsumer, TopicPartition
import threading

def consume_partition(consumer, partition):
    for message in consumer:
        print(f"Consumer in group {group_id} assigned to partition {message.partition()} received: {message.value.decode()}")

def create_consumer_and_thread(bootstrap_servers, group_id, topic, partition):
    consumer = KafkaConsumer(bootstrap_servers=bootstrap_servers, group_id=group_id)
    topic_partition = [(topic, partition)]
    consumer.assign(topic_partition)

    thread = threading.Thread(target=consume_partition, args=(consumer, partition))
    thread.start()
    return consumer, thread

def main():
    bootstrap_servers = 'your_bootstrap_servers'
    group_id = 'your_group_id'
    topic = 'your_topic'
    num_partitions = 8

    # Create consumers for each partition in the group
    consumers_and_threads = []
    for partition in range(num_partitions):
        consumer, thread = create_consumer_and_thread(bootstrap_servers, group_id, topic, partition)
        consumers_and_threads.append((consumer, thread))

    # Do some other work in the main thread if needed

    # Wait for all threads to finish
    for _, thread in consumers_and_threads:
        thread.join()

if __name__ == "__main__":
    main()





